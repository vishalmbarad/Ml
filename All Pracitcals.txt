------ DBSCAN ------


import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.cluster import DBSCAN

X, y = make_moons(n_samples=100, noise=0.05, random_state=42)

db = DBSCAN(eps=0.2, min_samples=5).fit(X)
labels = db.labels_

plt.figure(figsize=(10,8))
plt.scatter(X[:,0],X[:,1], c=labels, cmap='viridis',s=50)
plt.title("DBSCAN Clustering")
plt.show()




------ Hierarchical ------



import numpy as np
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from sklearn.datasets import make_blobs

X, y = make_blobs(n_samples=30, centers=3, cluster_std=0.7, random_state=42)

Z = linkage(X,method='ward')

plt.figure(figsize=(10,8))

dendrogram(Z, labels=np.arange(len(X)))

plt.title("Hierarchical Clustering Dendrogram")
plt.xlabel("Data Points")
plt.ylabel("Distance")
plt.show()


#cut tree into 3 clusters
clusters = fcluster(Z, t=3, criterion='maxclust')

plt.figure(figsize=(10,8))
plt.scatter(X[:,0],X[:,1],c=clusters,cmap='rainbow',s=50)
plt.title("Hierarchical Clustering ( 3 clusters )")
plt.show()




------ KMEDOID ------



import matplotlib.pyplot as plt
import numpy as np
from pyclustering.cluster.kmedoids import kmedoids
from pyclustering.utils import read_sample
from pyclustering.samples.definitions import SIMPLE_SAMPLES
from scipy.spatial import Voronoi, voronoi_plot_2d

sample = read_sample(SIMPLE_SAMPLES.SAMPLE_SIMPLE3)
print(sample)

initial_medoids = [1, 10, 20]

kmedoids_instance = kmedoids(sample, initial_medoids)
kmedoids_instance.process()

medoids = kmedoids_instance.get_medoids()
print("Medoids : ", medoids)

clusters = kmedoids_instance.get_clusters()
print("Clusters : ", clusters)

medoid_points = np.array([sample[m] for m in medoids])

plt.figure(figsize=(8, 6))
colors = ['blue', 'green', 'purple', 'orange', 'yellow']

for i, cluster in enumerate(clusters):
    cluster_points = np.array([sample[j] for j in cluster])
    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], s=30, c=colors[i % len(colors)], label=f'Cluster {i+1}')

plt.scatter(medoid_points[:, 0], medoid_points[:, 1], color='red', marker='X', s=200, label='Medoids')

plt.title("K-Medoids Clustering Result")
plt.legend()

vor = Voronoi(medoid_points)
voronoi_plot_2d(vor, show_vertices=False, ax=plt.gca(), line_colors='orange')

plt.show()




------ K-MEANS ------



import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
from scipy.spatial import Voronoi, voronoi_plot_2d

# 1. Generate Sample Data
X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=42)

# 2. Apply K-means
kmeans = KMeans(n_clusters=4, n_init=10, random_state=42)
kmeans.fit(X)

# 3. Get Centroids
centroids = kmeans.cluster_centers_
labels = kmeans.labels_

# 4. Plot K-means Cluster
plt.figure(figsize=(12,5))

plt.subplot(1, 2 ,1)
plt.scatter(X[:, 0], X[:, 1], c=labels, s=30, cmap='viridis')
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, label='Centroids')
plt.title("K-Means Clustering")
plt.legend()

# 5. Plot Voronoi Diagram
vor = Voronoi(centroids)

plt.subplot(1, 2 ,2)
voronoi_plot_2d(vor, show_vertices=True, show_points=True, ax=plt.gca())
plt.scatter(X[:, 0], X[:, 1], c=labels, s=30, cmap='viridis')
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, label='Centroids')
plt.title("Voronoi Diagram")
plt.legend()

plt.show()





------ Apriori ------



import mlxtend
from mlxtend.frequent_patterns import apriori, association_rules
import pandas as pd

transactions = [
    ['Milk','Bread','Eggs'],
    ['Milk','Bread'],
    ['Milk','Diaper','Beer', 'Bread','Eggs'],
    ['Bread','Diaper','Milk','Beer'],
    ['Bread','Milk','Diaper','Beer','Cola']
]

all_items = sorted(set(item for t in transactions for item in t))
print('all item : ',all_items)

rows = [{item: (item in t) for item in all_item }for t in transactions]
print('rows : ',rows)
df = df.DataFrame(rows)
print("One-Hot encoded dataset : ")
print(df)

frequent_itemssets = apriori(df, min_Support=0.6, use_colnames=True)

print("\nFrequent Itemsets (>60% Support) : ")





------ Linear Regression with pandas ------




import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression


df = pd.read_csv("D:\\ML\\advertising.csv")

X = df[['TV','Radio','Newspaper']]
y = df['Sales']

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state = 42)

model = LinearRegression()
model.fit(X_train,y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)


print("Intercept : ",model.intercept_)
print("Coefficients : ",model.coef_)
print(f"RMSE : {rmse:.3f}")
print(f"R2 : {r2:.4f}")

plt.scatter(y_test, y_pred,color="blue",alpha=0.7)
plt.plot([y.min(),y.max()],[y.min(),y.max()],'r--',linewidth=1.5)
plt.xlabel("Actual Sales")
plt.ylabel("Predicted Sales")
plt.title("Actual Sales vs Predicted Sales")
plt.show()




------ Logistic Regression ------



from sklearn.linear_model import LogisticRegression

X = [[25,40000],[35,60000],[45,80000],[20,20000]] #age,salary
y = [0,1,1,0] #1 will buy #0 will not buy

model = LogisticRegression()
model.fit(X,y)

print("Prediction (Buy or not) : ",model.predict([[35,60000]]))




------ Polynomial Regression ------



from sklearn.linear_model import LinearRegression
# from sklearn.preprocessing import PolynomialRegression
from sklearn.preprocessing import PolynomialFeatures
import numpy as np

x = np.array([[1],[2],[3],[4],[5]])
y = np.array([1,4,9,16,25])

poly = PolynomialFeatures(degree=2)
x_poly=poly.fit_transform(x)

model = LinearRegression()
model.fit(x_poly,y)

print("Predicted Price : ",model.predict(poly.transform([[6]])))



------ Multiple Regression ------



from sklearn.linear_model import LinearRegression
import numpy as np

x = [[1400,3],[1600,3],[1700,4],[1875,4]] #size,bedrooms
y = [245000,312000,279000,308000] #price

model = LinearRegression()
model.fit(x,y)

print("Predicted Price : ",model.predict([[1800,4]]))




------ Linear Regression ------



from sklearn.linear_model import LinearRegression
import numpy as np

x = np.array([[5],[10],[15],[20]]) #study hours
y = np.array([50,60,70,80]) #marks

model = LinearRegression()
model.fit(x,y)

print("Predicted Marks for 12 Hours : ",model.predict([[12]]))





------ SVM with Scatter plot ------



import numpy as np
from sklearn import datasets
from sklearn.svm import SVC
import matplotlib.pyplot as plt

iris = datasets.load_iris()
X = iris.data[:,:2]
y = iris.target

svm_model = SVC(kernel="linear",C=1.0)
svm_model.fit(X,y)

x_min,x_max = X[:,0].min() - 1,X[:,0].max() + 1
y_min,y_max = X[:,1].min() - 1,X[:,1].max() + 1
   
   
xx,yy = np.meshgrid(np.linspace(x_min,x_max,500),np.linspace(y_min,y_max,500))
   
Z = svm_model.predict(np.c_[xx.ravel(),yy.ravel()])

z = z.reshape(xx.shape)


plt.figure(figsize=(10,8))
plt.contourf(xx,yy,z,alpha=0.3,cmap=plt.cm.coolwarm)
plt.scatter(X[:,0],X[:,1],c=y,cmap=plt.cm.coolwarm,edgecolors='k')


plt.scatter(svm_model.support_vectors_[:,0], svm_model.support_vectors_[:,1],s=120,facecolors='none',edgecolors='k',linewidths=1.5,label="Support Vectors")

plt.title("SVM Decision Boundaries on IRIS Dataset (2 Features)")
plt.xlabel("Sepal Length")
plt.ylabel("Sepal Width")

plt.legend()
plt.show()


































------ SVM 29-7-2025 ------



from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

iris = load_iris()
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)

svm_model = SVC(kernel = "rbf", C=1.0 , gamma="scale")
svm_model.fit(X_train,y_train)

y_pred = svm_model.predict(X_test)

print("Accuracy : ",accuracy_score(y_test, y_pred))
print("\nClassification Report : \n", classification_report(y_test, y_pred, target_names = iris.target_names))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,4))
sns.heatmap(cm,annot=True,cmap="Blues",xticklabels=iris.feature_names,yticklabels=iris.target_names)

plt.title("Confusion Matrix")
plt.ylabel("Actual")
plt.show()
print("")




------ Random Forest ------




from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

iris = load_iris()
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)

rf = RandomForestClassifier(n_estimators=100,criterion="gini", max_depth=None, random_state = 42)
rf.fit(X_train,y_train)

y_pred = rf.predict(X_test)

print("Accuracy : ",accuracy_score(y_test, y_pred))
print("/nClassification Report : /n", classification_report(y_test, y_pred, target_names = iris.target_names))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,4))
sns.heatmap(cm,annot=True,cmap="Blues",xticklabels=iris.feature_names,yticklabels=iris.target_names)

plt.title("Confusion Matrix")
plt.ylabel("Actual")
plt.show()
print("")
importances = rf.feature_importances_
for feature, importance in zip(iris.feature_names, importances):
    print(f"{feature}: {importance:.4f}")
	
	
	
	
	
------ Decision Tree Entropy ------




from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt

iris = load_iris()
# print("Iris Dataset : ",iris)
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)

clf = DecisionTreeClassifier(criterion="entropy", max_depth=3, random_state = 42)
clf.fit(X_train,y_train)

y_pred = clf.predict(X_test)

print("Accuracy : ",accuracy_score(y_test, y_pred))
print("\nClassification Report : \n", classification_report(y_test, y_pred, target_names = iris.target_names))

plt.figure(figsize=(10,10))
plot_tree(clf, filled=True, feature_names = iris.feature_names, class_names = iris.target_names)
plt.title("Decision Tree Visualization using Entropy")
plt.show()




------ Decision Tree Gini ------


from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt

iris = load_iris()
# print("Iris Dataset : ",iris)
X = iris.data
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)

clf = DecisionTreeClassifier(criterion="gini", max_depth=3, random_state = 42)
clf.fit(X_train,y_train)

y_pred = clf.predict(X_test)

print("Accuracy : ",accuracy_score(y_test, y_pred))
print("/nClassification Report : /n", classification_report(y_test, y_pred, target_names = iris.target_names))

plt.figure(figsize=(10,10))
plot_tree(clf, filled=True, feature_names = iris.feature_names, class_names = iris.target_names)
plt.title("Decision Tree Visualization")
plt.show()



------ KNN ------



from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import pandas as pd

iris = load_iris()
# print("Iris Dataset : ",iris)
X = iris.data
y = iris.target
print("X : ",X)
print("y : ",y)

df = pd.DataFrame(iris.data,columns = iris.feature_names)
df['target'] = iris.target

df['flower_names'] = df['target'].apply(lambda x: iris.target_names[x])

#print(df.head(10)) #this will show first 10 records

print(df.tail(10))

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)
k=3

model = KNeighborsClassifier(n_neighbors=k)
model.fit(X_train,y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test,y_pred)
print("Accuracy : ",accuracy)

#new_data = [[5.1, 3.5, 1.4, 0.2]] # this is 1st line data
new_data1 = [[5.9, 3. , 5.1, 1.8]] # this is last line data
prediction = model.predict(new_data1)
predicted_class = iris.target_names[prediction[0]]
print("Predicted Class : ",predicted_class)



------ Market Basket Analysis ------



from sklearn.naive_bayes import CategoricalNB
from sklearn.feature_extraction import DictVectorizer
import numpy as np

user_purchases = [
    {"bread":1,"butter":1,"cheese":1},
    {"bread":1,"butter":1},
    {"bread":1,"cheese":1},
    {"butter":1,"cheese":1},
    {"bread":1},
    {"butter":1},
    {"cheese":1},
    {"bread":1,"butter":1,"milk":1},
    {"bread":1, "chips":1},
    {"butter":1,"cheese":1,"milk":1}
]

features = []
target_bread = []

for user in user_purchases:
    user_copy = user.copy()
    target_bread.append(1 if user_copy.pop('bread', 0) == 1 else 0)
    features.append(user_copy)

vec = DictVectorizer(sparse = False)
X = vec.fit_transform(features)
Y_bread = np.array(target_bread)

model_bread = CategoricalNB()
model_bread.fit(X, Y_bread)

new_user = [{'butter': 1,'cheese': 1}]
X_new = vec.transform(new_user)
prob_bread = model_bread.predict_proba(X_new)[0][1]

print("Probability pf buying bread: ", round(prob_bread * 100, 2), "%")
print("Expected out of 100 users: ", int(prob_bread * 100))

target_butter = []
features = []

for user in user_purchases:
    user_copy = user.copy()
    target_butter.append(1 if user_copy.pop('butter', 0) == 1 else 0)
    features.append(user_copy)
vec = DictVectorizer(sparse = False)
X = vec.fit_transform(features)
Y_butter = np.array(target_butter)

model_butter = CategoricalNB()
model_butter.fit(X, Y_butter)

X_new = vec.transform(new_user)
prob_butter = model_butter.predict_proba(X_new)[0][1]

print("Probability pf buying butter: ", round(prob_butter * 100, 2), "%")
print("Expected out of 100 users: ", int(prob_butter * 100))

expected_both = int(prob_bread * prob_butter * 100)
print("Probability of buying both bread and butter: ", expected_both , "/100 users")



------ Diease ------



p_disease = 0.01
p_no_disease = 0.99

p_positive_disease = 0.99
p_positive_no_disease = 0.05

p_positive = (p_positive_disease * p_disease) + (p_positive_no_disease * p_no_disease)

p_disease_positive = (p_positive_disease * p_disease) / p_positive

print(f"Probability of having disease given a positive test : {p_disease_positive:.2%}")



------ Fake Email Detection  ------


from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomailNB
from sklearn.metrics import accuracy_score

emails = [ "Win money now", "CLaim your free prize","Call now to win", "Meeting at 10pm", "Lunch tommorow?", "Project deadline extended" ]

vectorizer = CountVectorizer()
X= vectorizer.fit_transform(emails)

X_train , X_test, Y_train, Y_test = tain_test_split(X, labels, test_size=0.33, random_state=42)

predictions = model.predict(X_test)

accuracy = accuracy_score(y_test,predictions)
print("Accuracy  : ",accuracy_score(y_test, predictions))



-------------------------------------------------- END --------------------------------------------------